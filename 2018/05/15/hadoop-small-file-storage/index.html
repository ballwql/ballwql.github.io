<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Hadoop小文件存储方案 | ballen博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Hadoop小文件存储方案</h1><a id="logo" href="/.">ballen博客</a><p class="description">专注于智能运维&amp;大数据</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Hadoop小文件存储方案</h1><div class="post-meta">May 15, 2018<span> | </span><span class="category"><a href="/categories/hadoop/">hadoop</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><p>#前言<br>近来，业务部门因历史原因，希望对现存的图片、对账等历史文件进行改造，由原先的单机存储改成分布式存储便于管理和维护，目前组内也在大力推广HDFS在部门的应用 ，所以在此背景下，调研了目前关于HDFS的文件存储方案，本文会着重从小文件需求入手，分析目前各种现有小文件存储的状况及各自使用的场景。</p>
<p>#HDFS总体架构<br>在介绍文件存储方案之前，我觉得有必要先介绍下关于HDFS存储架构方面的一些知识，在对架构有初步了解后，才会明白为什么要单独针对小文件展开介绍，小文件存储和其它文件存储区别在什么地方。</p>
<p>这里我只是就Hadoop生态中的存储层展开介绍，对于其它部分本文暂未描述。众所周知，HDFS是目前非常流行的分布式文件存储系统，其逻辑架构如下图所示：<br><img src="/2018/05/15/hadoop-small-file-storage/hadoop_1x_structure.jpg" alt="HDFS架构"></p>
<p>HDFS也是典型的Master/Slave结构，其中，Master相当于Namenode，Slave相当于Datanode。</p>
<p>Namenode 负责元数据管理，维护文件和目录树，响应Client请求；Datanode负责实际数据存储。至于什么是元数据，是怎么管理的，可参考我的另一篇文章<a href="http://km.oa.com/group/11819/articles/show/319776" target="_blank" rel="external">Hadoop的HA机制</a>。</p>
<p>Block是文件块，HDFS中是以Block为单位进行文件的管理的，一个文件可能有多个块，每个块默认是3个副本，这些块分别存储在不同机器上。块与文件之前的映射关系会定时上报Namenode。HDFS中一个块的默认大小是64M，其大小由参数<code>dfs.block.size</code>控制。这里面先引申几个问题出来：</p>
<blockquote>
<ul>
<li><p><strong>问题1：块大小要怎么设置为一个合理值，过大设置和过小设置有什么影响？</strong></p>
</li>
<li><p><strong>问题2：如果一个文件小于所设置的块大小，实际占用空间会怎样？</strong></p>
</li>
<li><p><strong>问题3：一个Namenode最多能管理多少个块，什么时候会达到瓶颈？</strong></p>
</li>
</ul>
</blockquote>
<p>针对这些问题，后面会展开介绍，这里还是先关注下架构方面。针对块方面，有几个单位概念需要弄清楚： <strong>Block、Packet和Chunk</strong>。Block上面有描述，Packet和Chunk如下：</p>
<blockquote>
<ul>
<li><ol>
<li><strong>Packet</strong>: 其比块要小很多，可以理解为Linux操作系统最小盘块概念，一般为64KB，由参数<code>dfs.write.packet.size</code>控制，是client向Datanode写入数据的粒度，即client向Datanode写数据时不是一次以Block为单位写的，而是被分成若干Packet，放入pipeline顺序追加写入到Block中，示意图如下：</li>
</ol>
</li>
</ul>
</blockquote>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_packet.png" alt="packet"></p>
<blockquote>
<ul>
<li><ol>
<li><strong>Chunk</strong>: 比Packet更小，是针对Packet数据校验粒度来设计的，一般是512B,由参数<code>io.bytes.per.checksum</code>控制，同时还带有一个4B的校验值，所以可以认为一个Chunk是516B</li>
</ol>
</li>
</ul>
</blockquote>
<p>上面说到Chunk是针对数据校验的，那一个Packet有多少个chunk校验呢，如果Packet默认是64KB, 那计算公式为：<code>chunk个数=64KB/516B=128</code>。也就是对于一个Packet来说，数据值与校验值比例大概为<strong>128:1</strong>, 对于一个块来说，假设是64M，会对应512KB的校验文件。</p>
<p>Packet的示意图中还一个Header信息，实际存储的是Packet的元数据信息，包括Packet在block中的offset, 数据长度，校验编码等。</p>
<p>#HDFS写流程</p>
<p>了解块相关概念后，再介绍下HDFS的写入流程，如下图所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_write_process.png" alt="HDFS写流程"></p>
<blockquote>
<ul>
<li><ol>
<li>client向Namenode发起写文件RPC请求；</li>
</ol>
</li>
<li><ol>
<li>Namenode检查要写的文件是否已存在元数据中，存在则拒绝写入；同时检查写入用户权限，如无权限也拒绝写入；若文件不存在且有权限写入，则Namenode会创建一条文件记录，响应client端允许写入文件；</li>
</ol>
</li>
<li><ol>
<li>client根据文件大小分成若干块，并在Namenode中申请块所存放的Datanode位置，如果是3副本存储，则Namenode会选择3台符合条件的结点放到结点队列中；client实际向Datanode写数据时是以Packet为单位来写到Block的，这里面会涉及两个队列，分别为:<strong>data packet队列</strong>和<strong>ack packet队列</strong>，Packet会同时入数据队列和ack队列；</li>
</ol>
</li>
<li><ol>
<li>通过DataStreamer对象将数据写入pipeline中的第一个Datanode,并依次写入到其它两个结点；当三个结点packet都写成功后，会将packets 从ack queue中删除；</li>
</ol>
</li>
<li><ol>
<li>写操作完成后，client调用close()关闭写操作，并通知Namenode关闭写操作，至此，整个写操作完成。</li>
</ol>
</li>
</ul>
</blockquote>
<p>packet写入流示意图如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_packet_flow.png" alt="packet流程图"></p>
<p>#HDFS读流程<br>HDFS的读流程比较简单，流程程如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_read_process.png" alt="HDFS读流程"></p>
<blockquote>
<ul>
<li><ol>
<li>client 向Namenode发起读文件RPC请求；</li>
</ol>
</li>
<li><ol>
<li>Namenode返回相应block所在datanode的位置信息；</li>
</ol>
</li>
<li><ol>
<li>client通过位置信息调用FSDataInputStream API的Read方法从datanode中并行读取block信息，如图中4和5所示，选择block的其中一副本返回client。</li>
</ol>
</li>
</ul>
</blockquote>
<p>#HDFS块信息介绍<br>在对HDFS的读写流程有一个基础了解后，下面针对文件块存储相关内容展开介绍。了解块的设计、存储和元数据相关知识对于设计小文件存储方案也至关重要。</p>
<h2 id="HDFS块设计原则"><a href="#HDFS块设计原则" class="headerlink" title="HDFS块设计原则"></a>HDFS块设计原则</h2><p>有人可能会问，集群存储有大文件也有小文件，那块大小该如何设计呢，这里应该要考虑2个准则：</p>
<blockquote>
<ul>
<li><p>1.减少内存占用：对于Namenode来说，单机内存毕竟有限，文件块越多，元数据信息越大，占用内存越多，如果文件数量级很大的话，单机将无法管理；</p>
</li>
<li><p>2.减少硬盘寻道时间： 数据块在硬盘为连续存储，对于普通SATA盘，随机寻址较慢， 如果块设置过小，一个文件的块总数会越多，意味着硬盘寻址时间会加长，自然吞吐量无法满足要求；如果块设置过大，一方面对于普通盘来说IO性能也比较差，加载时会很慢，另一方面，块过大，对于多副本来说，在副本出问题时，系统恢复时间越长。</p>
</li>
</ul>
</blockquote>
<p>所以设置合理的块大小也很重要，一般来说根据集群的需求来设定，比如对于使用到HBase的场景，一般数据量会比较大，块不宜设置太小，参考值一般为128MB或256MB，这样能尽量避免频繁块刷写和块元数据信息的膨胀；对于存储小文件的场景，如图片，块可设置成默认64MB大小，一个块中存储多个图片文件，后面会详细介绍。</p>
<h2 id="HDFS块存储原则"><a href="#HDFS块存储原则" class="headerlink" title="HDFS块存储原则"></a>HDFS块存储原则</h2><p>块在HDFS中是怎么存储的呢，上面有提到多副本机制，即一个块在HDFS中是根据<strong>dfs.replication</strong>参数所设置的值来确定副本数的，默认为3。三个副本是随机存储三台数据结点Datanode上，三个结点的选取遵循机架感知策略，通过<strong>topology.script.file.name</strong>来设置， 如果配置中未配置机架感知，Namenode是无法知道机房网络拓扑，所以会随机选取3台结点进行块存储，如果设置了机架感知，则在存储时会在同机架存储2副本，不同机架放第3个副本，这样一旦一个机架出现问题，还能保证一个副本是可用的。</p>
<p>如果一个文件只有几K，且小于HDFS块大小，实际在HDFS占用的空间会是多少呢？答案是文件大小即为实际占用空间，对于几K的文件实际占用的空间大小也为几K，不会占用一个块空间。</p>
<h2 id="HDFS块元数据信息"><a href="#HDFS块元数据信息" class="headerlink" title="HDFS块元数据信息"></a>HDFS块元数据信息</h2><p>上面提到，在存储的文件数量级很大时，单机Namenode内存消耗会急剧增大，易触发单机瓶颈，那么到底一个Namenode可以管理多少量级的元数据呢，其实这个可以有一个公式来初略估算。这里首先要了解一个概念，元数据包括哪些，正常元数据包括三个部分：<strong>文件、目录和块</strong>。这三部分在元数据中各占用多少空间呢，下面是一个初略的计算：</p>
<blockquote>
<ul>
<li><ol>
<li>单条元数据大小：文件约250B，目录约290B，块约368B(152B+72*副本数3)</li>
</ol>
</li>
<li><ol>
<li>集群元数据总条数：文件数约10000个，目录约5000个，块约20000个</li>
</ol>
</li>
<li><ol>
<li>总占用内存大小： 250B<em>10000+290B</em>5000+368B*20000=10.78M</li>
</ol>
</li>
</ul>
</blockquote>
<p>实际内存消耗会比这多，因为还有其它一些信息需要存储，总体内存消耗可根据上述公式来估算，这样你就知道你集群Namenode能承受多少文件，目录和块元数据信息的存储。也能及时发现内存瓶颈，做到精细化监控运营管理。</p>
<p>上述介绍的三个方面也分别解答了上面提到的三个问题，具体细节这里也不过多展开。下面正式展开对小文件存储方面的介绍</p>
<h1 id="HDFS小文件存储方案"><a href="#HDFS小文件存储方案" class="headerlink" title="HDFS小文件存储方案"></a>HDFS小文件存储方案</h1><p>针对小文件问题，HDFS自身也有考虑这种场景，目前已知的主要有三种方案来实现这种存储，分别如下：</p>
<blockquote>
<ul>
<li>HAR</li>
<li>SequenceFile</li>
<li>CombinedFile</li>
</ul>
</blockquote>
<h2 id="HAR存储方案"><a href="#HAR存储方案" class="headerlink" title="HAR存储方案"></a>HAR存储方案</h2><p>HAR熟称Hadoop归档文件，文件以*.har结尾。归档的意思就是将多个小文件归档为一个文件，归档文件中包含元数据信息和小文件内容，即从一定程度上将Namenode管理的元数据信息下沉到Datanode上的归档文件中，避免元数据的膨胀。</p>
<p>归档文件是怎么生成的呢，主要还是依赖于MapReduce原理将小文件内容进行归并。归档文件的大概组成如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_har.png" alt="HDFS HAR组成"></p>
<p>图中，左边是原始小文件，右边是har组成。主要包括：<code>_masterindex</code>、<code>_index</code>、<code>part-0...part-n</code>。其中_masterindex和_index就是相应的元数据信息，part-0…part-n就是相应的小文件内容。实际在集群中的存储结构如下：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_har_1.png" alt="HDFS HAR原文件"></p>
<p>通过<code>hadoop archive</code>命令创建归档文件，-archiveName指定文件名, -p指定原文件路径，-r指定要归档的小文件,最后指定hdfs中归档文件存放路径，如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_har_2.png" alt="HAR创建"></p>
<p>创建后，会在/usr/archive目录下生成test.har目录，这里大家可能会有疑惑，上面不是说Har是一个文件吗，这里怎么又是目录了呢，其实我们所说的归档文件是逻辑上的概念，而实际的har是一个目录，是一个物理存储概念，所以大家只要记住在实际存储时生成的Har实际上是一个目录就行了。这个目录中会存放元数据，实际文件内容。如下图所示，_index文件的每一行表示的是小文件在part开头的映射关系，包括起始和结束位置，是在哪个part文件等，这样在读取har中的小文件时，根据offset位置可直接得到小文件内容，如图part-0文件内容所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_har_3.png" alt="HAR结构"></p>
<p>要从HAR读取一个小文件的话，需要用distcp方式，原理也是mapreduce, 指定har路径和输出路径，命令如下：<br><code>hadoop distcp har:///user/archive/test.har/file-1 /tmp/archive/output</code></p>
<p>HAR总体比较简单，它有什么缺点呢?</p>
<blockquote>
<ul>
<li><p>1.archive文件一旦创建不可修改即不能append，如果其中某个小文件有问题，得解压处理完异常文件后重新生成新的archive文件;</p>
</li>
<li><p>2.对小文件归档后，原文件并未删除，需要手工删除;</p>
</li>
<li><p>3.创建HAR和解压HAR依赖MapReduce，查询文件时耗很高;</p>
</li>
<li><p>4.归档文件不支持压缩。</p>
</li>
</ul>
</blockquote>
<h2 id="SequenceFile存储方案"><a href="#SequenceFile存储方案" class="headerlink" title="SequenceFile存储方案"></a>SequenceFile存储方案</h2><p>SequenceFile本质上是一种二进制文件格式，类似key-value存储，通过map/reducer的input/output format方式生成。文件内容由Header、Record/Block、SYNC标记组成，根据压缩的方式不同，组织结构也不同，主要分为Record组织模式和Block组织模式。</p>
<p>###Record组织模式</p>
<p>Record组织模式又包含两种：未压缩状态CompressionType.NONE, 和压缩状态CompressionType.RECORD，未压缩是指不对数据记录进行压缩，压缩态是指对每条记录的value进行压缩，其逻辑结构如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_sf_record.png" alt="Record模式"></p>
<p>Record结构中包含Record长度、key长度、key值和value值。Sync充斥在Record之间，其作用主要是用于文件位置定位，具体定位方式是：如果提供的文件读取位置不是记录的边界可能在一个Record中间，在实际定位时会定位到所提供位置处之后的第一个Sync边界位置，并从该Sync点往后读相应长度的数据，如果提供的读取位置往后没有Sync边界点，则直接跳转文件末尾；如果提供的文件读取位置是Record边界，则直接从该位置开始读取指定长度的数据。另一种文件定位方式是seek, 这种方式则要求所提供的读取位置是record的边界位置，不然在读取迭代读取下一个位置时会出错。</p>
<p>###Block组织模式<br>Block组织模式，其压缩态为CompressionType.BLOCK。与Record模式不同的时，Block是以块为单位进行压缩，即将多条Record写到一个块中，当达到一定大小时，对该块进行压缩，很显然，块的压缩效率会比Record要高很多，避免大量消费IO和CPU等资源。其逻辑结构如下：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_sf_block.png" alt="Block模式"></p>
<p>从上图中可看出，组织方式变成了块，一个块中又包含了块的记录数，key长度，key值，value长度，value值。每个块之间也有Sync标记，作用同Record方式。</p>
<p>两中模式中，都有header标记，包含了些如版本信息、KEY类名、VALUE类名、是否压缩标记、是否块压缩标记、编码类、元数据信息和Sync标记，其结构如下：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_sf_header.png" alt="header结构"></p>
<p>###SequenceFile示例<br>这里以存储5个小的图片文件为例，演示下如何创建SequenceFile。首先将图片文件上传至hdfs的一个目录。</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_sf_ex_1.png" alt="图片文件示例"></p>
<p>其次，编写一个MR程序来对上述图片进行转换，将生成的文件存放到/tmp/sequencefile/seq下，MR程序源码在附件SmallFiles.zip中，可自行查看，如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_sf_ex_2.png" alt="MR程序转换"></p>
<p>转换后，会在/tmp/sequencefile/seq目录生成一个part-r-00000文件，这个文件里面就包含了上述5个图片文件的内容，如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_sf_ex_3.png" alt="SequenceFile目录结构"></p>
<p>如果要从该SequenceFile中获取所有图片文件，再通过MR程序从文件中将图片文件取出，如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hdfs_sf_ex_4.png" alt="SequenceFile取文件"></p>
<p>###SequenceFile优缺点</p>
<p>####优点</p>
<blockquote>
<ul>
<li>A.支持基于记录或块的数据压缩;</li>
<li>B. 支持splitable,能够作为mr 的输入分片;</li>
<li>C. 不用考虑具体存储格式，写入读取较简单.</li>
</ul>
</blockquote>
<p>####缺点</p>
<blockquote>
<ul>
<li>A. 需要一个合并文件的过程</li>
<li>B. 依赖于MapReduce</li>
<li>C. 二进制文件，合并后不方便查看</li>
</ul>
</blockquote>
<p>##CombinedFile存储方案<br>其原理也是基于Map/Reduce将原文件进行转换，通过CombineFileInputFormat类将多个文件分别打包到一个split中，每个mapper处理一个split, 提高并发处理效率，对于有大量小文件的场景，通过这种方式能快速将小文件进行整合。最终的合并文件是将多个小文件内容整合到一个文件中，每一行开始包含每个小文件的完整hdfs路径名，这就会出现一个问题，如果要合并的小文件很多，那么最终合并的文件会包含过多的额外信息，浪费过多的空间，所以这种方案目前相对用得比较少，下面是使用CombineFile的示例：</p>
<figure class="highlight plain"><figcaption><span>ls</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">hbaseadmin@10-163-161-229:~/program/mr/input&gt; touch file-1 file-2 file-3</div><div class="line">hbaseadmin@10-163-161-229:~/program/mr/input&gt; echo &quot;this is file-1&quot; &gt;file-1</div><div class="line">hbaseadmin@10-163-161-229:~/program/mr/input&gt; echo &quot;this is file-2&quot; &gt;file-2</div><div class="line">hbaseadmin@10-163-161-229:~/program/mr/input&gt; echo &quot;this is file-3&quot; &gt;file-3</div><div class="line">hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -put input /tmp/combinefile/</div><div class="line"></div><div class="line">hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop jar SmallFiles.jar  com.fit.dba.mr.util.CombineFileTest /tmp/combinefile/input/ /tmp/combinefile/output</div><div class="line"></div><div class="line">hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -ls /tmp/combinefile/output</div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   1 hbaseadmin supergroup          0 2018-03-25 17:26 /tmp/combinefile/output/_SUCCESS</div><div class="line">-rw-r--r--   1 hbaseadmin supergroup        213 2018-03-25 17:26 /tmp/combinefile/output/part-r-00000</div><div class="line">hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -ls /tmp/combinefile/output/part-r-00000</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   1 hbaseadmin supergroup        213 2018-03-25 17:26 /tmp/combinefile/output/part-r-00000</div><div class="line">hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -cat /tmp/combinefile/output/part-r-00000</div><div class="line">hdfs://10-163-161-229:9000/tmp/combinefile/input/file-1 this is file-1</div><div class="line">hdfs://10-163-161-229:9000/tmp/combinefile/input/file-2 this is file-2</div><div class="line">hdfs://10-163-161-229:9000/tmp/combinefile/input/file-3 this is file-3</div></pre></td></tr></table></figure>
<p>上述用到的转换程序也在附件CombineFileTest.java中。其优点是适用于处理大量比block小的文件和内容比较少的文件合并，尤其是文本类型/sequencefile等文件合并，其缺点是：如果没有合理的设置maxSplitSize，minSizeNode，minSizeRack，则可能会导致一个map任务需要大量访问非本地的Block造成网络开销，反而比正常的非合并方式更慢。</p>
<p>##总结<br>上面介绍了三种基于HDFS自身的一些方案，每种方案各有优缺点，其核心思想都是基于map/reduce的方式将多个文件合并成一个文件。在实际使用中，单纯用上述方案还是不太方便，下面简要介绍下目前其它的一些小文件存储方案。</p>
<p>#其它小文件存储方案</p>
<p>##基于HBase的小文件存储方案<br>HBase我们知道主要是key/value存储结构，一个key对应多个列族的多个列值。从2.0版本开始，HBase多了一个MOB的结构，具体参考HBase-11339。具体是什么概念呢，先来看下示意图：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hbase_sf_mob.png" alt="hbase架构"></p>
<p>上图是一个关于HBase的架构图，包含HBase的几个组件master、regionserver、hdfs、hfile等。MOB FILE类似StoreFile, 它作为一个单独的对象存储小文件。MOB具体结构如下所示：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hbase_sf_mob_structure.png" alt="mob结构"></p>
<p>MOB是由StoreFile和MOB File共同组成。其中，StoreFile存放的数据和HBase正常存储的数据一样，key/value结构，不过value中存储的是关于MOB文件的长度，存放路径等元数据信息，在MOB File中存储的是具体的MOB文件内容，这样通过StoreFile中的key/value可以找到MOB所存放的文件具体位置和大小，最终得到文件内容。</p>
<p>MOB是怎么设置呢，在创建表时我们指定表的MOB属性，如下所示：<br><code>create &#39;t1&#39;, {NAME =&gt; &#39;f1&#39;, IS_MOB =&gt; true, MOB_THRESHOLD =&gt; 102400}</code></p>
<p>其中，MOB_THRESHOLD表示MOB对象所能存储的文件对象上限阈值，推荐是存储小于10M的文件。对于MOB的表，我们可以手动触发压缩，有<strong>compact_mob</strong>和<strong>major_compact_mob</strong>两种方式。如下所示：<br><code>compact_mob &#39;t1&#39;
compact_mob &#39;t1&#39;,&#39;cf1&#39;
major_compact_mob &#39;t1&#39;
major_compact_mob &#39;t1&#39;,&#39;cf1&#39;</code></p>
<p>MOB的出现大大提高了我们使用HBase存储小文件的效率，这样无须关注底层HDFS是怎么存储的，只要关注上层逻辑即可，HBase的强大优势也能保证存储的高可靠和稳定性，管理也方便。</p>
<p>##基于打包构建索引方案<br>这种方案是目前兄弟部门正在使用的一个小图片存储方案，也是基于HDFS存储实际图片，基于HBase存储元数据信息。这个方案中，主要也是基于压缩的思路，将多个小图上片压缩成一个tar文件存放至HDFS上，通过HBASE记录文件名和HDFS文件的位置映射关系，架构示意图如下：</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/hbase_sf_tar.png" alt="架构示意图"></p>
<p>其具体思路是：</p>
<blockquote>
<ul>
<li>1.业务部门将图片上传至一个中转机，图片按日期目录存储，不同日期上传的图片放到相应日期目录；</li>
<li>2.定期用脚本去将日期目录打包成tar，一天的图片打包成一个以日期命名的tar, tar文件解压后是直接图片文件，即不带日期那层目录，上传至HDFS指定目录；</li>
<li>3.通过tar文件解析程序获取tar文件中各图片文件在tar中的偏移量和长度，这个解析程序最开始是由国外一个程序员Tom Wallroth写的工具,具体地址可以访问:<a href="http://github.com/devsnd/tarindexer。" target="_blank" rel="external">http://github.com/devsnd/tarindexer。</a> 这个工具可以直接在tar文件上解析tar中各文件的偏移量和长度，很方便。</li>
<li>4.得到图片文件在tar的偏移量和长度后，设计HBase rowkey, 将图片名和tar路径设计到rowkey中，并通过在rowkey前缀加盐方式使rowkey随机散列分布在HBase中，避免热点现象；HBase的value存储的是文件的偏移量和长度。这样HBase中就保存了文件的元数据信息；</li>
<li><ol>
<li>业务方查询具体某个图片时，根据图片的日期和图片名，先计算出HBase rowkey,再去HBase获取该图片的偏移量和长度；通过偏移量和长度通过HDFS的API去读取HDFS的文件。</li>
</ol>
</li>
</ul>
</blockquote>
<p>##其它方案对比<br>下面针对目前行业内用到的其它一些方案作下对比，如下图所示。</p>
<p><img src="/2018/05/15/hadoop-small-file-storage/sf_compare.png" alt="方案对比"> </p>
<p>总体来说，淘宝的TFS是功能最全的，同时支持大小文件的存储；Ceph也是一种流行的分布式文件存储方案，组内对其调研后感觉比较复杂，不太好管理，不太稳定；FastDFS比较简单，适合存储一些使用场景简单的文件，不太灵活；其它几种没用过，大家可自行上网参阅相关资料。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文介绍了关于HDFS小文件存储的方案，不同方案各具特点，在使用时要根据实际业务场景进行设计，对于既要存储大文件又要存储小文件的场景，我建议在上层作一个逻辑处理层，在存储时先判断是大文件还是小文件，再决定是否用打包压缩还是直接上传至HDFS，可借鉴TFS方案。</p>
<p>#参考<br><a href="hadoop-small-file-storage/hadoop_1x_structure.jpg">1</a> <a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="external">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</a></p>
<p>#附件<br>关于文章中用的MR转换程序如下所示：<br><a href="https://github.com/ballwql/common-tool/tree/master/java" target="_blank" rel="external">https://github.com/ballwql/common-tool/tree/master/java</a></p>
<p><strong>版权声明：本文为博主原创文章，未经博主允许不得转载。</strong></p>
</div><div class="tags"><a href="/tags/hadoop/">hadoop</a></div><div class="post-nav"><a href="/2018/05/15/hdfs-multi-replicas-analysis/" class="next"></a></div><div id="container"></div><link rel="stylesheet" href="/css/default.css?v=0.0.0"><script src="/js/gitment.browser.js?v=0.0.0"></script><script>var gitment = new Gitment({
  owner: 'ballwql## Your GitHub ID, e.g. username',
  repo: 'ballwql.github.io## The repository to store your comments, make sure you're the repo's owner, e.g. imsun.github.io',
  oauth: {
    client_id: 'e1067ce8b7aaa1a88a45## GitHub client ID, e.g. 75752dafe7907a897619',
    client_secret: '5f4e1d8d82c95a8610d5d19d51e97b7926fd5e71## GitHub client secret, e.g. ec2fb9054972c891289640354993b662f4cccc50',
  },
})
gitment.render('container')
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://zendwind.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/HBase/">HBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/python/" style="font-size: 15px;">python</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/15/hadoop-small-file-storage/">Hadoop小文件存储方案</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/15/hdfs-multi-replicas-analysis/">hdfs-multi-replicas-analysis</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/11/phoenix-introduction/">浅谈Phoenix在HBase中的应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/15/python-read-dump/">Python解析mysqldump文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/11/hbase-read-write-exception-summary/">HBase读写异常问题总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/08/hadoop-ha-qjm/">Hadoop HA机制学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/27/first-blog/">开篇导言</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.cnblogs.com/ballwql" title="Cnblogs" target="_blank">Cnblogs</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">ballen博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zIndex="-2" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>